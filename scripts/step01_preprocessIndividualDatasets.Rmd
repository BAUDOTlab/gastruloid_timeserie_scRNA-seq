---
title: "01_Data preprocessing - Analysis datasets"
author: "Céline Chevalier, Anaïs Baudot, Fabienne Lescroart, Laurent Argiro, Stéphane Zaffran"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document: 
    theme: cerulean
    number_sections: yes
    code_folding: hide
    df_print: kable
    toc: yes
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
    keep_tex: no
editor_options:
  chunk_output_type: inline
  markdown: 
    wrap: sentence
urlcolor: blue
header-includes:
  - \usepackage{fvextra}
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines, commandchars=\\\{\}}
---

```{r setup01, include=FALSE}
options(knitr.purl.inline = TRUE)
knitr::opts_chunk$set(echo=TRUE,
                      eval=TRUE,
                      results="hide",
                      tidy=TRUE, # wrap long code lines
                      tidy.opts=list(width=105) # maximum width of the line before being wrapped
                      )
knitr::opts_chunk$set(fig.pos='H', # a character string for the figure position arrangement to be used in \begin{figure}[]
                      fig.show='hold' # hold all plots and output them at the end of a code chunk
                      )
```

```{r libraries, include=FALSE}
library(dplyr)
library(matrixStats)
library(Seurat)
library(BiocGenerics)
library(ggplot2)
library(edgeR)
library(ggrepel)
library(reticulate)
library(Rmagic)
library(phateR)
library(scmap)
library(scater)
#library(harmony)
library(plotly)
library(uwot)
library(RColorBrewer)
library(viridis)
library(DoubletFinder)
#library(future.batchtools)
library(future.apply)
library(stringr)
library(grid)
library(gridExtra)
library(kableExtra)
library(tidyr)
```

\clearpage

# Introduction

<!-- ************** -->

<!-- Speak about gastruloids -->

<!-- ************** -->

The major parts of the following analyse have been inspired by the pipeline used in the article [Rossi et al. Cell Stem Cell 2020](https://doi.org/10.1016/j.stem.2020.10.013).
The associated code is available on github, at the [Cardiac_Gastruloids](https://github.com/nbroguiere/Cardiac_Gastruloids) repository.

The idea of the analysis is to analyze and compare the data obtained in the lab to the ones retrieved from the Rossi et al. article.
The data used in the Rossi et al. article will be named "reference data" in the rest of the document.
These data were recovered from the [GEO website](https://www.ncbi.nlm.nih.gov/geo/) with the accession number GEO: GSE158999.
The accessed files are:

-   barcodes.tsv.gz,

-   features.tsv.gz,

-   matrix.mtx.gz,

-   metadata.tsv.gz.

The Rossi et al. article also refers and uses the *in vivo* atlas from [Pijuan-Sala Griffiths Gottgens et al. Nature 2019](https://doi.org/10.1038/s41586-019-0933-9).
Indeed, in Rossi et al, a classifier has been trained on the atlas data.
I directly download the classifier from the above mentioned github repository.

I also downloaded the data of the atlas of Pijan-Sala et al. using a terminal with the following command lines

```{bash load_atlas, eval=FALSE}
curl https://content.cruk.cam.ac.uk/jmlab/atlas_data.tar.gz > atlas_data.tar.gz
tar -zxvf atlas_data.tar.gz
```

\clearpage

# General settings

## Directories managment

To reproduce the analysis, it is recommended to set up all these directories.
I assume that all the input files are stored in the same directory `inputData`.
This directory is divided in 5 subdirectories:

-   2 of them are dedicated to the raw data to be analyzed, from the Marseille Medical Genetics (MMG) laboratory and reference datasets (`zaffranRawData` and `rossiRawData`, respectively),

-   1 directory contains the data to create an atlas object from Pijuan-Sala et al. (`atlas`),

-   1 directory stores the classifier provided by the reference Rossi et al. based on the atlas of Pijuan-Sala et al. (`scmap_pijuansala_classifier`),

-   1 directory contains input tables provided by the reference Rossi et al. (`InputTables`).

```{r directoriesExe, include=FALSE}
basePath <- "/shared/projects/mothard_in_silico_modeling/seurat_analysis/"
# input paths
atlas.directory <- paste0(basePath, "inputData/atlas/")
classifier.folder <- paste0(basePath, "inputData/scmap_pijuansala_classifier/")
inputTables.folder <- paste0(basePath, "inputData/InputTables/")

# output paths
baseAnalysis <- paste0(basePath, "2022-06-09_completeAnalysis_remoteIFB/")
if(!dir.exists(baseAnalysis)){dir.create(baseAnalysis)}
rdsObjects <- paste0(baseAnalysis, "rdsObjects/")
if(!dir.exists(rdsObjects)){dir.create(rdsObjects)}
atlas.folder <- paste0(baseAnalysis, "atlas/")
if(!dir.exists(atlas.folder)){dir.create(atlas.folder)}
fig.folder <- paste0(baseAnalysis, "figures/")
if(!dir.exists(fig.folder)){dir.create(fig.folder)}
```

```{r setup02, include=FALSE}
knitr::opts_chunk$set(fig.path=fig.folder, dev='png')
```

```{r directoriesPrinted, eval=FALSE}
basePath <- "XXXXX" # to be seen as the root for any analysis based on a set of input data
# input paths
atlas.directory <- paste0(basePath, "inputData/atlas/")
classifier.folder <- paste0(basePath, "inputData/scmap_pijuansala_classifier/")
inputTables.folder <- paste0(basePath, "inputData/InputTables/")

# output paths
baseAnalysis <- paste0(basePath, "XXXXX")
if(!dir.exists(baseAnalysis)){dir.create(baseAnalysis)}
rdsObjects <- paste0(baseAnalysis, "rdsObjects/")
if(!dir.exists(rdsObjects)){dir.create(rdsObjects)}
atlas.folder <- paste0(baseAnalysis, "atlas/")
if(!dir.exists(atlas.folder)){dir.create(atlas.folder)}
fig.folder <- paste0(baseAnalysis, "figures/")
if(!dir.exists(fig.folder)){dir.create(fig.folder)}
```

<!-- ## Functions loaded from source -->

<!-- To work around compatibility issues, some functions come from a local script file. -->

<!-- ```{r sourcedFunctions} -->

<!-- source(paste0(basePath, "scripts/RunHarmony.R")) -->

<!-- source(paste0(basePath, "scripts/writeClipboard.R")) -->

<!-- ``` -->

## Load custom color tables

To keep a coherent coloration on the plots through the analysis, I set up a vector of colors.

```{r colors}
colors.table <- read.table(file=paste0(inputTables.folder, "ClusterColors.tsv"), sep="\t", header=T, comment.char="", as.is=T)
colors.use_ab.initio <- setNames(colors.table$blind_friendly[!is.na(colors.table$ab_initio_identity)], colors.table$ab_initio_identity[!is.na(colors.table$ab_initio_identity)])
colors.use_transferred <- setNames(colors.table$blind_friendly[!is.na(colors.table$transferred_identity)], colors.table$transferred_identity[!is.na(colors.table$transferred_identity)])
colors.use_stages <- setNames(c("#be465c", "#b04b3d","#c86633", "#d0a63f", "#998138", "#90b03d", "#60a756", "#45c097", "#5e8bd5", "#6d71d8", "#573585", "#bd80d5", "#b853a2", "#ba4b7d"), c("Day_04", "Day_05", "Day_06", "Day_07", "Day_10", "E6.5", "E6.75", "E7.0", "E7.25", "E7.5", "E7.75", "E8.0", "E8.25", "E8.5"))
```

## Custom variables for the analysis

Some following functions can work with parallelization.
Here I configure how parallelization should work.

```{r commonVar}
# Parallelization
plan(strategy = "multisession", workers=4)
#plan(batchtools_slurm, template = "/shared/projects/mothard_in_silico_modeling/slurm_scripts/aa_seuratAn.tmpl")
options(future.globals.maxSize = 62914560000) #60GB
options(future.seed = 26)
```

\clearpage

# Reference data

## Load the reference dataset

The `Read10X` function only needs the directory in which the input files have been stored.
It creates an object interpreting the barcodes, features and matrix files.\
The `CreateSeuratObject` function initialize a seurat object.

<!--# It also permits to filter cells and genes according to a lower threshold of gene expression. Here I kept genes expressed in at least 3 cells and cells with at least 100 genes detected. -->

```{r refDataLoad, results='hold'}
ref.dataset.folder <- paste0(basePath, "inputData/ref_data/")

ref.raw.data <- Read10X(data.dir=ref.dataset.folder, gene=1)
ref.4d.SO <- CreateSeuratObject(counts=ref.raw.data, project="recoverAnalysis") #, assay="RNA", min.cells=3, min.features=100)

# Dimensions of the object
head(ref.4d.SO)
dim(ref.4d.SO)
```

The reference dataset contains `r dim(ref.4d.SO)[[2]]` cells on which `r dim(ref.4d.SO)[[1]]` features (genes) were detected.

## Add metadata

The metadata file was created in the Rossi et al. analysis, and provided to guide users.
I add the `stage` and `Replicate` metadata to the seurat object under the names `day` and `replicate` column name respectively.

```{r refAddMetaD}
ref.md <- read.table(file=paste0(ref.dataset.folder, "metadata.tsv.gz"), sep="\t", header=TRUE, stringsAsFactors=F)
ref.4d.SO <- AddMetaData(object=ref.4d.SO, metadata=ref.md$stage, col.name="day")
ref.4d.SO <- AddMetaData(object=ref.4d.SO, metadata=ref.md$Replicate, col.name="replicate")
```

Here are all the metadata information contained in the seurat object of the reference dataset: `str(ref.4d.SO@meta.data)`

```{r md_01, echo=FALSE, results='hold'}
str(ref.4d.SO@meta.data)
```

There are 5 metadata stored in a data frame.
Each of them describe the cells.
Besides the imported metadata from the external file (`day` and `replicate`), there are `n_Count_RNA` and `nFeature_RNA` information:

-   `nCount_RNA` is the number of reads counted in every droplet with a gel bead associated to a cell,

-   `nFeature_RNA` is the number of different features (genes) detected in every droplet.

We can get the different values a metadata can take by the following command.
Here is an example for the metadata named `day`: `unique(ref.4d.SO@meta.data$day)`

```{r md_02, echo=FALSE, results='hold'}
unique(ref.4d.SO@meta.data$day)
```

### Homogenize labels (`day`, `dataset`)

For the sake of the analysis, I need to manipulate the labels to make them coherent between all the labels.

```{r homogenize_ref}
# modify day label
ref.4d.SO@meta.data$day <- gsub('^(Day)([0-9]*)$', '\\1_0\\2', ref.4d.SO@meta.data$day)

# create new metadata named dataset
ref.4d.SO$dataset <- paste0('ref_', tolower(ref.4d.SO@meta.data$day))
Idents(ref.4d.SO) <- ref.4d.SO$dataset

ref_dataset_names <- unique(ref.4d.SO$dataset)
```

A new metadata has been created.
The values this metadata can take are among `r unique(ref.4d.SO$dataset)` depending on the origin and the stage (day) of the sequenced cells.

Now here's what the metadata looks like:

`r data.frame(head(ref.4d.SO@meta.data)) %>%
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))`

## Split into sub-datasets

For the following parts of the analysis, I will split the dataset `ros.4d.SO` into 4 sub-datasets.
I use the `SplitObject` function from the Seurat package (PROVIDE REFERENCE).
By providing an attribute (metadata column name), the object is splitted into a list of subsetted objects.

I split the dataset according to the `day` the gastruloids were sequenced.

```{r splitRefData}
# Split the object
refSO.list <- SplitObject(ref.4d.SO, split.by = 'dataset')

# set up the project name of each sub-dataset
for (SO in refSO.list){
    projName <- unique(SO@meta.data$dataset)
    refSO.list[[projName]]@project.name <- projName
}
```

Size of the entire dataset:

`r data.frame("ref_4_days", dim(ref.4d.SO)[[1]], dim(ref.4d.SO)[[2]]) %>% 
  setNames(c('Dataset', 'Nbr of features', 'Nbr of cells')) %>%
  knitr::kable(align = "lrr") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))`
  
Size of sub-datasets:

`r data.frame(cbind(sapply(refSO.list, function(SO) SO@project.name), t(sapply(refSO.list, dim)))) %>% 
  setNames(c('Dataset', 'Nbr of features', 'Nbr of cells')) %>%
  knitr::kable(align = "lrr") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))`

\clearpage

# MMG laboratory data

The lab got the scRNAseq data day by day.
The sequencing saturation was not good enough for the day 5.
As the consequence, this day has been resequenced.
It results in that I have five datasets: one dataset for the days 4, 6 and 10 and two datasets for the day 5.

For each dataset, I will execute the following steps:

-   load and interprete raw data (barcodes, features and matrix files) with `Read10X` function, and create a seurat object (`CreateSeuratObject` function),

-   add metadata.

## Load the 5 datasets

There are five sub-datasets to load.
I create a seurat object for each of them that I store in a list.
The project name of each sub-dataset is already given.

```{r labDataLoad, results='hold'}
# get path to the folders of each sub-dataset
lab_dirs <- list.files(paste0(basePath, "inputData/lab_data"), full.names = TRUE)
print(lab_dirs)

# create a list of seurat objects
labSO.list <- list()
lab_dataset_names <-c()
for (x in lab_dirs){
    dataset.name <- str_extract(x, '[a-z0-9_]*$')
    lab_dataset_names <- append(lab_dataset_names, dataset.name)
    
    raw.data <- Read10X(data.dir=x, gene=2)
    SO <- CreateSeuratObject(counts=raw.data, project=dataset.name)#, assay="RNA", min.cells=3, min.features=100)

    labSO.list[[dataset.name]] <- SO
}

labSO.list
```

As well as for the project names for the reference sub-datasets, the project name of the lab sub-datasets take the values among: `r lab_dataset_names`.

## Add metadata

I need to add metadata like for the reference dataset.
I will add the information of the day the gastruloids were sequenced, the dataset name - like the reference dataset name were built - and the replicate.

The replicate metadata is required as there are two datasets sequenced on the day 5.
It will be useful to differenciate cells between both datasets.

```{r labAddMetaD, results="hold"}
labSO.list <- lapply(labSO.list,
                     function(SO){
                         dataset <- SO@project.name
                         day <- str_to_title(str_extract(dataset, 'day_[0-9]*$'))
                         
                         SO <- AddMetaData(object = SO, metadata = day, col.name = "day")
                         SO <- AddMetaData(object = SO, metadata = dataset, col.name = "dataset")
                         
                         if (dataset == 'lab_day_05bis'){
                             SO <- AddMetaData(SO, metadata = 1, col.name = 'replicate')
                         } else {
                             SO <- AddMetaData(SO, metadata = 0, col.name = 'replicate')
                         }
                     }
)

data.frame(head(labSO.list[[1]]@meta.data)) %>%
  knitr::kable(align = "lrrrrrr") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

# Nomenclature for cell names

The names of the cells have a different nomenclature between the reference and the MMG laboratory sub-datasets.
On one hand, the reference cell names are already custom names with the biological origin (`GAS` for gastruloids), the days the gastruloid were sequenced, the batch number and the UMI.
On the other hand, the laboratory cell names only consists on the UMIs.

The library used by 10X Genomics is limited, and provide a limited number of unique UMI barcodes.
(PROVIDE DETAILS).
Hence, working with data obtained from multiple sequencing experiments might lead to an overlap fo UMI barcodes between datasets.

Hence, I decide to create a nomenclature to rename all the cells under a unique identifier.
Thus, each cell will be identified by

-   ref/lab whether the data come from Rossi et al. or the MMG laboratory,

-   the dataset value previously added to the metadata,

-   the replicate value, also present in the metadata and

-   the UMI provided by 10x Genomics.

```{r renaming_cells, results='hold'}
rename_cells <- function(SO){
    cell_ids <- colnames(SO)
    UMIs <- str_extract(cell_ids, '[A-Z]*$')
    cellnames <- paste(SO$dataset, SO$replicate, UMIs, sep = '_')
    
    SO <- RenameCells(SO, new.names = cellnames)
}

refSO.list <- lapply(refSO.list, rename_cells)
labSO.list <- lapply(labSO.list, rename_cells)
print(colnames(refSO.list$ref_day_04)[1])
print(colnames(labSO.list$lab_day_04)[1])
```

We can see that there is as many unique cell identifiers as the number of cells among all the sub-datasets.

```{r uniqCellID_ref, results='hold'}
nbCells <- Reduce("+" , lapply(refSO.list, function(SO) dim(SO)[2])) +
    Reduce("+" , lapply(labSO.list, function(SO) dim(SO)[2]))

nbIDs <- Reduce("+", lapply(refSO.list, function(SO) length(unique(colnames(SO))))) +
    Reduce("+", lapply(labSO.list, function(SO) length(unique(colnames(SO)))))

print(paste('Nbr of cells among all the sub-datasets:', nbCells))
print(paste('Nbr of cell identifiers:', nbIDs))
```

# - - -

\clearpage

Now, all the sub-datasets are prepared for the analysis.
First, I will proceed to the quality control of the cells, after what I will realize the standard preprocessing workflow of Seurat.
Then, I will be able to remove the doublets using DoubletFinder library (PROVIDE SOURCE).
Finally, I will investigate if there is sources of unwanted variation among the replicates.

First and foremost, I gather all the data into a common object.
Thus, they are stored in a list.
The computational advantage of it takes place in the use of the function of `lapply()` .
It allows to apply a function to all members of a list in one command.\
All the sub-datasets (reference and lab data) will be analyzed on the same pipeline.

```{r allSOlist, warning=FALSE, message=FALSE, results='hold'}
allSO.list <- do.call(c, list(refSO.list, labSO.list))
# allSO.list <- append(refSO.list, labSO.list) # Equivalent
# allSO.list <- c(refSO.list, labSO.list) # Equivalent
SO.names <- names(allSO.list)
rm(refSO.list, labSO.list)
gc(verbose = FALSE)
str(allSO.list, max.level = 2)
```

Now, all sub-datasets Seurat objects are stored into a list.
The names of the list are the `project.name` of each sub-dataset.

As the number of cells could decrease during the following steps, I will print a table with the number of cells, but also the number of detected features in each sub-dataset.
A barplot will be also displayed to see the evolution of each sub-dataset along the analysis.

Size of all the sub-datasets:

```{r size_track02, results='hold', rows.print=5}
size_suball <- data.frame(cbind(
    sapply(allSO.list, function(SO){
        if (str_detect(SO@project.name, "^ref")){ "ref" } else { "lab" }}),
    sapply(allSO.list, function(SO){
        if (str_detect(SO@project.name, "04")) { "4" }
        else if (str_detect(SO@project.name, "05bis")) { "5bis" }
        else if (str_detect(SO@project.name, "05")) { "5" }
        else if (str_detect(SO@project.name, "06")) { "6" }
        else if (str_detect(SO@project.name, "07")) { "7" }
        else if (str_detect(SO@project.name, "11")) { "11" }
    }),
    t(sapply(allSO.list, dim)),
    "Preliminary Counts",
    "01"
))
colnames(size_suball) <- c('origin', 'day', 'Nbr_of_features', 'Nbr_of_cells', 'Analysis_step_name', 'Step_number')
size_suball$Nbr_of_cells <- as.integer(as.character(size_suball$Nbr_of_cells))
size_suball$Nbr_of_features <- as.integer(as.character(size_suball$Nbr_of_features))
size_suball$day <- factor(size_suball$day, levels = c('4', '5', '5bis', '6', '7', '11'), ordered = TRUE)
size_suball %>%
  knitr::kable(align = "lrrrrlr") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

```{r hist_counts01, fig.show='hold'}
ggplot(size_suball, aes(x = day, y = Nbr_of_cells)) +
    geom_col(aes(color = origin, fill = origin), position = position_dodge(), width = 0.7) +
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5)) +
    ggtitle("Number of cells in each sub-dataset") +
    facet_grid(.~ origin, scales = "free_x", space = "free")
```

# Quality control

The cells can be filtered out based on multiple criteria, like:

-   the number of features detected in a cell transcriptome,

-   the number of counts for each single cell or,

-   the percentage of mitochondrial counts.

Thus, the feature plot of the `nCount_RNA` an the `percent.mt` metadata will guide me regarding the threshold to apply for filtering out the low quality cells.

## Percentage of mitochondrial reads & cutoffs

To determine the mitochondrial percentage in each cell, I use the mitochondrial gene name that always begins by "mt-".
The function `PercentageFeatureSet` enables to calculate the percentage of all counts that match the pattern.
Here, I use the "mt-" pattern.
(REFORMULATE)

```{r addMitoPercentage, results='hide'}
allSO.list <- future_lapply(
    allSO.list, 
    function(SO){
        SO[["percent.mt"]] <- PercentageFeatureSet(SO, pattern="^mt-")
        return(SO)
    },
    future.seed = 26
)
```

The percentage of mitochondrial counts (or reads) is stored for each cell as a metadata under the name of `percent.mt` (see the example on the data of the lab, day 4).

`r data.frame(head(allSO.list$lab_day_04@meta.data)) %>%
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))`

```{r message=FALSE, out.width="30%", warning=FALSE, fig.show='hold'}
lapply(allSO.list, function(SO) {
    VlnPlot(SO, features = c("percent.mt"), ncol = 3) +
        geom_hline(aes(yintercept = 10), color="blue", size=1.5) +
        geom_hline(aes(yintercept = 1.5), color="orange", size=1.5)
})
```

The reference sub-datasets show a mitochondrial content that never exceeds 15% of the transcripts in cells.
In addition, cells never have less than 1.5% of mitochondrial content.
It is the reason why there is no orange line on the 4 first plots (plots on reference sub-datasets).
One can suggest that the data were already filtered.
In contrast, some cells in the lab data have a mitochondrial content close to 80%.

I will keep the cells whose percentage is between 1.5% and 10% (orange and blue lines, resp.).

Information coming from [10X Genomics](https://kb.10xgenomics.com/hc/en-us/articles/360001086611-Why-do-I-see-a-high-level-of-mitochondrial-gene-expression-)

> Apoptotic cells express mitochondrial genes and export these transcripts to the cytoplasm in mammalian cells.
> Lysed cells with intact mitochondria may be partitioned into GEMs, increasing the fraction of mitochondrial transcripts detected.

It is also mentioned in [Luecken MD and Theis FJ *Mol Syst Biol*. 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6582955/) that a high mitochondrial content may represent doublets.

## Counts' thresholds

<!--# {r mt_plot, out.height="20%", fig.show='hold', include=FALSE, eval=FALSE} -->

<!--# #Plot the mitochondrial content in function of the number of counts in the cells  -->

<!--# future_lapply(allSO.list, function(x) FeatureScatter(object=x, feature1="nCount_RNA", feature2="percent.mt") + ggtitle(paste0("Level of mitochondrial expression (in %)\nby cells\n", x@project.name)) + NoLegend(), future.seed = 26) -->

Rossi et al. was filtering out cells with a number of read counts lower than 2000 (green line).
Added to this one, I will also remove the cells with a number of read counts higher than 150.000 (purple line).

```{r message=FALSE, out.width="30%", warning=FALSE, fig.show='hold'}
lapply(allSO.list, function(SO) {
    VlnPlot(SO, features = c("nCount_RNA"), ncol = 3) +
        geom_hline(aes(yintercept = 150000), color="purple", size=1.5) +
        geom_hline(aes(yintercept = 3000), color="green", size=1.5)
})
```

The reference sub-datasets never have less than 3000 read counts.
It is the reason why there is no purple line on the 4 first plots (plots on reference sub-datasets).
Actually, it appears that they never have less than 5000 read counts.

Furthermore, the higher threshold leads to filter out some outliers either in reference and laboratory sub-datasets.
The 2 datasets of the laboratory fifth day sequencing do not reach such number read counts.

## Minimum of feature's diversity

The number of feature per cell is also assessed for quality control.

```{r message=FALSE, out.width="30%", warning=FALSE, fig.show='hold'}
lapply(allSO.list, function(SO) {
    VlnPlot(SO, features = c("nFeature_RNA"), ncol = 3) +
        geom_hline(aes(yintercept = 1800), color="#56B4E9", size=1.5)
})
```

The reference sub-datasets never have less than 2000 features per cell, while the lab sub-datasets range from `r min(sapply(allSO.list[5:9], function(x) min(x@meta.data$nFeature_RNA)))` to `r max(sapply(allSO.list[5:9], function(x) max(x@meta.data$nFeature_RNA)))` features per cell.
Many cells of the lab sub-datasets show a low number of detected features.
Thus, a minimum of 1800 detected features per cell will be requested (blue line).

## Isolated features

Among the thousands of detected features, some of them are expressed in few cells.
Such features do not provide information on the cell heterogeneity.
It is also good to know that filtering out features expressed in less than 50 cells (red dashed line) will make difficult to detect clusters that could gather less than 50 cells.

```{r message=FALSE, out.width="30%", warning=FALSE, fig.show='hold'}
# Create dataframe
countcells.df.list <- lapply(allSO.list, function(SO) {
    df <- data.frame(rowSums(SO@assays$RNA@counts != 0))
    df$features <- rownames(df)
    colnames(df) <- c('Nbr_of_cells', 'features')
    rownames(df) <- NULL
    return(df)
})

lapply(1:length(countcells.df.list), function(i) {
  project.name <- names(countcells.df.list)[i]
  df <- countcells.df.list[[i]]
    # Plot gene expression repartition
    ggplot(df, aes(x = Nbr_of_cells)) +
        geom_histogram() +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5)) +
        ylab('frequency') +
        scale_x_continuous(trans = "log10") +
        expand_limits(x=c(0,10500), y=c(0,2000)) +
        geom_vline(aes(xintercept = 8), color="green", size=1) +
        geom_vline(aes(xintercept = 50), color="red", size=1, linetype="dashed") +
        ggtitle(paste0("Repartition of expressed features\n", project.name))
})
    
```

In order to limit the number of dropouts, and still, being able to identify small clusters, the features expressed in less than 8 cells will be removed.

## Apply QC

Considering the multiple quality control criteria all together, I confirm the previously settled thresholds.
The low quality cells will be removed.

```{r message=FALSE, out.width="30%", warning=FALSE, fig.show='hold'}
# lapply(allSO.list, function(SO) {
#     print(names(SO@meta.data))
#     FeatureScatter(object=SO, feature1="nCount_RNA", feature2="nFeature_RNA", pt.size=0.8, cols='percent.mt') +
#         geom_point(data = data.frame(SO@meta.data$nCount_RNA, SO@meta.data$nFeature_RNA, SO@meta.data$percent.mt), aes(x = 'nCount_RNA', y = 'nFeature_RNA'), color='percent.mt') +
#         geom_hline(aes(yintercept = 1500), color="#56B4E9", size=1) +
#         geom_vline(aes(xintercept = 150000), color="purple", size=1) +
#         geom_vline(aes(xintercept = 3000), color="green", size=1) +
#         ggtitle(paste0("Nb of features as function of Nb of counts\n", SO@project.name)) +
#         scale_color_continuous(type = "viridis")
# })

lapply(allSO.list, function(SO) {
    ggplot(SO@meta.data, aes(nCount_RNA, nFeature_RNA, colour = percent.mt)) +
        geom_point() +
        lims(colour = c(0, 100)) +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5)) +
        ylab('nFeature_RNA') +
        geom_hline(aes(yintercept = 1800), color="#56B4E9", size=1) +
        geom_vline(aes(xintercept = 150000), color="purple", size=1) +
        geom_vline(aes(xintercept = 3000), color="green", size=1) +
        ggtitle(paste0("Nb of features as function of Nb of counts\n", SO@project.name))
})

```

The cells with a high content of mitochondrial expressed genes are under the blue line (minimum of 1800 features), and also at the left of the green line (3.000 read counts minimum).
Such cell with a low count depth (count per cell), few detected features and a high mitochondrial content are indicative of cells whose cytoplasmic mRNA has leaked out through a broken membrane, and thus, only mRNA located in the mitochondria is still conserved ([Luecken MD and Theis FJ *Mol Syst Biol*. 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6582955/)).

In this section, I apply all the cutoffs determined in the previous steps.
As a result, the sub-datasets will have either less features and cells.

```{r filterQC}
allSO.list <- lapply(1:length(allSO.list), function(i) {
  df <- countcells.df.list[[i]]
  SO <- allSO.list[[i]]
  SO <- subset(SO,
               features=which(df$Nbr_of_cells > 8),
               subset = percent.mt > 1.5 & percent.mt < 10 & nCount_RNA > 3000 & nCount_RNA < 150000 & nFeature_RNA > 1800)
  return(SO)
})
names(allSO.list) <- c(ref_dataset_names, lab_dataset_names)
```

Size of all the sub-datasets after filtering:

```{r size_track03, results='hold', rows.print=10}
size_suball2 <- data.frame(cbind(
    sapply(allSO.list, function(SO){
        if (str_detect(SO@project.name, "^ref")){ "ref" } else { "lab" }
    }),
    sapply(allSO.list, function(SO){
        if (str_detect(SO@project.name, "04")) { "4" }
        else if (str_detect(SO@project.name, "05bis")) { "5bis" }
        else if (str_detect(SO@project.name, "05")) { "5" }
        else if (str_detect(SO@project.name, "06")) { "6" }
        else if (str_detect(SO@project.name, "07")) { "7" }
        else if (str_detect(SO@project.name, "11")) { "11" }
    }),
    
    t(sapply(allSO.list, dim)),
    "QC_filtering",
    "02"
))
colnames(size_suball2) <- c('origin', 'day', 'Nbr_of_features', 'Nbr_of_cells', 'Analysis_step_name', 'Step_number')
size_suball2$Nbr_of_cells <- as.integer(as.character(size_suball2$Nbr_of_cells))
size_suball2$Nbr_of_features <- as.integer(as.character(size_suball2$Nbr_of_features))
size_suball2$day <- factor(size_suball2$day, levels = c('4', '5', '5bis', '6', '7', '11'), ordered = TRUE)
size_suball_track <- rbind(size_suball, size_suball2)
size_suball_track %>%
  knitr::kable(align = "lrrrrlr") %>% 
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  scroll_box(width = "100%", height = "200px")
rm(size_suball, size_suball2)
```

```{r hist_counts02}
ggplot(size_suball_track, aes(x = day, y = Nbr_of_cells)) +
    geom_col(aes(color = Analysis_step_name, fill = Analysis_step_name), position = position_dodge(), width = 0.6) +
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5)) +
    ggtitle("Number of cells in each sub-dataset\nafter QC") +
    facet_grid(.~ origin, scales = "free_x", space = "free")
```

SAY SOMETHING THAT ROSSI ET AL DATA WERE ALREADY PREPROCESSED

```{r include = FALSE}
gc()
```

# Seurat Standard Preprocessing Workflow

To compare the gene expression across multiple cells, the data have to be normalized.
I use the default parameter of the `NormalizeData` function.
It means that the method used is called *LogNormalize*.
With this method, the feature counts for each cell are divided by the total counts that cell and multiplied by the scale.factor (10.000, by default).
This is then natural-log transformed using log1p.

```{r normalization}
allSO.list <- future_lapply(allSO.list, NormalizeData, verbose=FALSE, future.seed = 26)
```

The normalization is calculated from the raw counts slot named **`counts`**.
The result is then stored in an another slot named **`data`**.

```{r results='hold'}
data.frame(GetAssayData(allSO.list$ref_day_04, slot = "counts")[1:3,1:5]) %>%
  knitr::kable(caption = "Raw counts in 'counts' slot") %>% 
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  scroll_box(width = "800px", height = "200px")
data.frame(GetAssayData(allSO.list$ref_day_04, slot = "data")[1:3,1:5]) %>%
  knitr::kable(caption = "Normalized counts in 'data' slot") %>% 
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  scroll_box(width = "800px", height = "200px")
```

I continue with the standard preprocessing steps, using default parameters:

-   highly variable features (HVG) detection, with the function `FindVariableFeatures` , where 2000 features are identified as having the most cell-to-cell variation,

-   scale the data, via a linear transformation, using the `ScaleData` function, on all features (more details below),

-   perform linear reduction dimension with Principal Component Analysis (PCA) to identify the sources of heterogeneity in every sub-dataset, through 50 principal components (PCs),

-   plot an ElbowPlot to visualize the most informative PCs.

```{r std_wf, warning=FALSE}
allSO.list <- future_lapply(allSO.list,
                     function(SO){
                         SO <- FindVariableFeatures(SO, nfeatures=2000, verbose=FALSE)
                         SO <- ScaleData(SO, features=rownames(SO), verbose=FALSE)
                         SO <- RunPCA(SO, verbose=FALSE)
                         return(SO)
                         },
                     future.seed = 26
                     )
```

The function `ScaleData` performs a linear transformation (so called *scaling*).
In detail, it shifts gene expressions to make the mean at 0 (negative values arise), and scales the gene expressions to have their variance equal to 1.
Concretely, it standardizes the expression of each gene.
This step gives equal weight in downstream analyses, so that highly-expressed genes do not dominate (\<<https://satijalab.org/seurat/articles/pbmc3k_tutorial.html>\>).

From the scaling step, a new slot named `scale.data` has been created.
Below is an example of the same features in the same cells as above.

```{r results='hold'}
data.frame(GetAssayData(allSO.list$ref_day_04, slot = "scale.data")[1:3,1:5]) %>%
  knitr::kable(caption = "Scaled data in 'scale.data' slot") %>% 
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  scroll_box(width = "800px", height = "200px")
```

The PCA was performed with the identified HVG as input.
The *Elbow plots* below show for each sub-datasets how much each PC is informative - in term of the percentage of variance.

In the analysis realized by Rossi et al., the PCA was done to compute 30 PCs only.
All the downstream analysis was considering all the PCs.
Here, I use the default parameters of the `RunPCA` function.
Thus, it computes 50 PCs.

```{r out.width="33%"}
lapply(allSO.list, function(SO) {
    ElbowPlot(SO, ndims = 50) +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5, size = 20)) +
        geom_vline(aes(xintercept = 30), color="purple", size=1) +
        ggtitle(paste0(SO@project.name, "\nElbowPlot"))        
})
```

By taking into account the advises from the [Seurat - Guided Clustering Tutorial vignette](https://satijalab.org/seurat/articles/pbmc3k_tutorial.html), the downstream analysis will be done on the 30 first PCs for all the sub-datasets.

Then, I go through the community detection among the cells.
The edges are constructed using a k-nearest neighbors (KNN) graph, based on the euclidean distances previously obtained on the PCA space.
The weights of the edges are based on the shared overlap in their local neighborhood, method call Jaccard similarity.
These 2 steps are done by using the `FindNeighbors` function.
\hfill [Seurat - Guided Clustering Tutorial vignette](<https://satijalab.org/seurat/articles/pbmc3k_tutorial.html>)

Once I obtain a graph on the data, I run the `FindClusters` function.
It applies the Louvain algorithm (by default).
This algorithm optimizes the modularity of the graph.

-   Modularity allows to assess the best division of a network (or graph). It also depends on a resolution parameter. Higher the value of the resolution, higher the number of clusters. \*

Originally, the analysis realized by Rossi et al. set the resolution parameter at 4.6.
Here, I test the Louvain algorithm according to multiple levels of resolution (0.1, 0.5, 1, 2 and 5).

```{r}
res <- c(0.1, 0.5, 1, 2, 5)
allSO.list <- future_lapply(allSO.list,
                     function(SO){
                         SO <- FindNeighbors(SO, dims = 1:30, verbose=FALSE)
                         SO <- FindClusters(SO, resolution = res, random.seed = 11, verbose=FALSE)
                         return(SO)
                         },
                     future.seed = 26
                     )
```

The relationship between the resolution's levels and the number of clusters is presented below.

```{r results='hold'}
data <- c()
resolution <- c()
nb_clusters <- c()
for (SO in allSO.list){
  for (ares in res){
    nb_levels <- nlevels(SO@meta.data[[paste0("RNA_snn_res.", ares)]])
    
    data <- append(data, SO@project.name)
    resolution <- append(resolution, ares)
    nb_clusters <- append(nb_clusters, nb_levels)
  }
}

df <- data.frame(data, resolution, nb_clusters)
df %>%
  pivot_wider(names_from="data", values_from="nb_clusters") %>%
  knitr::kable(caption = "Number of clusters in sub-datasets\naccording to the resolution level") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

By running a non-linear dimension reduction such as the Uniform Manifold Approximation and Projection (UMAP), the clusters can be visualized.

```{r message=FALSE, warning=FALSE, out.width="50%", fig.show='hold'}
allSO.list <- lapply(allSO.list, RunUMAP, dims = 1:30, verbose = FALSE)

for (ares in res){
  print(DimPlot(allSO.list$lab_day_11, group.by = paste0("RNA_snn_res.", ares), label = TRUE, reduction = "umap") +
          ggtitle(paste0(allSO.list$lab_day_11@project.name, "\nResolution level: ", ares)) +
          theme(plot.title = element_text(hjust=0.5)))
}
```

For the sub-dataset `day_11` of the laboratory, it appears that the best value for the resolution parameter should be at 1.
I set the `Idents` of all sub-datasets to the clusters defined under the resolution level 1.

```{r message=FALSE, out.width="50%", fig.show='hold'}
allSO.list <- lapply(allSO.list,
                     function(SO){
                       Idents(SO) <- SO$RNA_snn_res.1
                       print(DimPlot(SO, label = TRUE, reduction = "umap") +
                         ggtitle(paste0(SO@project.name, "\nResolution level: 1")) +
                         theme(plot.title = element_text(hjust=0.5)))
                       return(SO)
                     })
gc()
```

\clearpage

# Remove doublets

In single-cell RNA sequencing (scRNAseq) analysis, the cells in suspension are captured in a droplet with barcoded microparticles (REFERENCE TO MACOSKO 2015).
In most cases, a droplet contains one microparticle and one cell.
But sometimes, two or more cells might be captured within a droplet.
In that case, we are in presence of a doublet or multiplet.

Doublets are considered as technical artifacts that have to be filtered out.
In this part, I will use the R package DoubletFinder (REFERENCE PAPIER DOUBLET FINDER).

This tool require the following steps

-   homotypic doublet proportion estimation,

-   pK parameter optimization

to be completed.

## Heterotypic doublet estimation

Two types of doublets have been identified:

-   homotypic doublets, that derived from transcriptionally similar cells,

-   heterotypic doublets, that derived from transcriptionally distinct cells.

In fact, DoubletFinder is sensitive to the heterotypic doublets, but not to the homotypic ones (from [DoubletFinder github repository](https://github.com/chris-mcginnis-ucsf/DoubletFinder)).

To better identify the heterotypic doublets, it is neccessary to rely on cell annotation.

### Cell annotation

As the pipeline of Rossi et al., I will provide a celltype annotation on each cells.
So, I directly load the classifier done for 1165 gene markers.
This classifier was generated by the Rossi et al. using the data from the Pijuan-Sala et al. atlas.
I prefer to warn you that this classifier has nothing related to artificial intelligence classifiers.

```{r pijuanSalaClassifier, eval=TRUE}
scmap_classifier <- readRDS(file=paste0(classifier.folder, "scmap_classifier_1000markers.rds"))
ref_stages <- c("E6.5", "E6.75", "E7.0", "E7.25", "E7.5", "E7.75", "E8.0", "E8.25", "E8.5")
```

I use the scmap package from [Kiselev, V., Yiu, A. & Hemberg, M., Nat Methods 2018](https://www.nature.com/articles/nmeth.4644).

```{r ctClassifier, warning=FALSE, message=FALSE, out.width="50%"}
allSO.list <- lapply(allSO.list,
                     function(SO){
                       set.seed(1234567)
                       
                       # create the coresponding SingleCellExperiment object
                       sce <- as.SingleCellExperiment(x=SO)
                       rowData(sce)$feature_symbol <- rownames(sce)
                       counts(sce) <- as.matrix(counts(sce))
                       logcounts(sce) <- as.matrix(logcounts(sce))
                       
                       # apply scmapCluster
                       scmapCluster_results <- scmapCluster(projection=sce, index_list=scmap_classifier[ref_stages], threshold=0)
                       
                       # add the celltype from the in vivo atlas to the Seurat object
                       SO <- AddMetaData(object=SO, metadata=scmapCluster_results$combined_labs, col.name="celltype_DF")
                       
                       # save memory, do garbage collection
                       rm(sce)
                       gc()
                       
                       return(SO)
                     })

lapply(allSO.list,
       function(SO){
         Idents(SO) <- SO$celltype_DF
         DimPlot(SO, pt.size = 0.8, label = T, label.size = 6, repel = T,
                 cols = colors.use_transferred[levels(Idents(SO))]) +
           theme_void() +
           NoLegend() +
           ggtitle(SO@project.name) +
           theme(plot.title = element_text(hjust = 0.5, size = 20))
       }
)
```

### Homotypic doublet proportion estimation

As mentioned earlier, DoubletFinder is only sensitive to heterotypic doublets. By obtaining the proportion of homotypic doublets, I prevent DoubletFinder from overestimating the doublets to be removed across cells.

```{r warning=FALSE, out.width="50%", fig.show='hold', message=FALSE}
homo_prop <- sapply(allSO.list,
                        function(SO){
                          # based on annotation (celltype from pijuansala classifier)
                          # could be also the louvain clusters determined from the preprocessus step
                          annotations <- SO@meta.data$celltype_DF
                          homotypic.prop <- modelHomotypic(annotations)

			  return(homotypic.prop)
                        })
```

Now, I have an estimated number of homotypic doublets proportion in each of the datasets.

`r data.frame(homo_prop) %>%
  setNames(c('Homotypic doublets proportion')) %>%
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))`

### Heterotypic estimated doublets

The percentage of doublets is dependant of the number of cells loaded to the device before sequencing, and the prevalence of cell aggregates within the cell suspension.

In the Rossi et al. analysis, this rate was estimated at 7.6%
Here, according to the [user guide relative to our sequencing analysis](https://support.10xgenomics.com/single-cell-gene-expression/index/doc/user-guide-chromium-single-cell-3-reagent-kits-user-guide-v31-chemistry-dual-index), and given that arround 20.000 cells were loaded for each experiment, I will use the rate 8.0% on the lab data.

So, I apply the percentage of multiplets on the number of cells in each of the sub-datasets. Then, to estimate the number of heterotypic doublets, I multiply the result by the proportion of heterotypic proportion (`1 - homotypic doublet proportion`).

```{r warning=FALSE, message=FALSE}
dblts_nonhomo <- sapply(1:length(allSO.list),
                        function(i) {
                          # based on annotation (celltype from pijuansala classifier)
                          # could be also the louvain clusters determined from the preprocessus step
                          SO <- allSO.list[[i]]
                          homotypic.prop <- homo_prop[[i]]
                          
                          if (any(grepl("ref", SO@project.name))) {
                          	nDoublets <- round(ncol(SO)*7.6/100)
                          } else {
                          	nDoublets <- round(ncol(SO)*8/100)
                          }
                          
                          nDoublets_nonhomo <- round(nDoublets*(1-homotypic.prop))

			  return(nDoublets_nonhomo)
                        })
```

Below, ther is the estimated number of heterotypic doublets to remove.

`r data.frame(dblts_nonhomo) %>%
  setNames(c('Heterotypic estimated doublets')) %>%
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))`


## Parameters - pK identification

DoubletFinder relies on 3 main parameters:

-    pN, the number of artificial generated doublets - by default = 0.25

-    pK, the PC neighborhood size used to compute proportion of artificial nearest neighbors (pANN), to be determined

-    nExp, the number of predicted doublets, obtained from the previous step.

It has been shown that the pN parameter has no influence on the tool efficiency. As, I already determined the number of doublets to remove during the previous part, I just have to determine the pK parameter for each sub-dataset.

```{r warning=FALSE, out.width="50%", fig.show='hold', message=FALSE}
pK.list <- sapply(allSO.list,
                  function(SO){
                    sweep.res.list_SO <- paramSweep_v3(SO, PCs = 1:30) # as estimated from PC elbowPlot
                    sweep.stats_SO <- summarizeSweep(sweep.res.list_SO, GT = FALSE)
                    bcmvn_SO <- find.pK(sweep.stats_SO)
                    
                    ggplot(bcmvn_SO, aes(pK, BCmetric, group = 1)) +
                      geom_point() +
                      geom_line()
                    
                    pK <- bcmvn_SO %>% # select the pK that corresponds to max bcmvn to optimize doublet detection
                      filter(BCmetric == max(BCmetric)) %>%
                      select(pK) 
                    pK <- as.numeric(as.character(pK[[1]]))
                    return(pK)
                  })
```


```{r warning=FALSE, out.width="50%", fig.show='hold', message=FALSE}
## pK Identification (no ground-truth) ---------------------------------------------------------------------------------------



## Load pijuansala classifier (/!\ not AI) -----------------------------------------------------------------------------------

## Annotate cells from pijuansala classifier ---------------------------------------------------------------------------------


## Homotypic Doublet Proportion Estimate -------------------------------------------------------------------------------------


# run doubletFinder ---------------------------------------------------------------------------------------------------------- 
allSO.list <- lapply(allSO.list,
                     function(SO){
                       SO <- doubletFinder_v3(SO, 
                                              PCs = 1:30, 
                                              pN = 0.25, 
                                              pK = pK.list[[SO@project.name]], 
                                              nExp = dblts_nonhomo[[SO@project.name]])
                     })


# remove the cells tagged as doublet -----------------------------------------------------------------------------------------
allSO.list <- lapply(allSO.list,
                     function(SO){
                       # remove the cells identified as doublets of each of the sub-datasets
                       col_dblts <- grep("DF.classifications", colnames(SO@meta.data), value=TRUE)
                       Idents(SO) <- col_dblts
                       SO <- subset(SO, idents='Singlet')
                       
                       # remove useless columns
                       SO@meta.data[[grep("DF.classifications", colnames(SO@meta.data))]] <- NULL
                       names(SO@meta.data)[names(SO@meta.data) == grep("pANN",colnames(SO@meta.data),value=T)] <- "pANN"
                       
                       return(SO)
                     })


## Check size of the subdatasets ---------------------------------------------------------------------------------------------
size_suball3 <- data.frame(cbind(
    sapply(allSO.list, function(SO){
        if (str_detect(SO@project.name, "^ref")){ "ref" } else { "lab" }
    }),
    sapply(allSO.list, function(SO){
        if (str_detect(SO@project.name, "04")) { "4" }
        else if (str_detect(SO@project.name, "05bis")) { "5bis" }
        else if (str_detect(SO@project.name, "05")) { "5" }
        else if (str_detect(SO@project.name, "06")) { "6" }
        else if (str_detect(SO@project.name, "07")) { "7" }
        else if (str_detect(SO@project.name, "11")) { "11" }
    }),
    
    t(sapply(allSO.list, dim)),
    "Removed Doublets",
    "03"
))
colnames(size_suball3) <- c('origin', 'day', 'Nbr_of_features', 'Nbr_of_cells', 'Analysis_step_name', 'Step_number')
size_suball3$Nbr_of_cells <- as.integer(as.character(size_suball3$Nbr_of_cells))
size_suball3$Nbr_of_features <- as.integer(as.character(size_suball3$Nbr_of_features))
size_suball3$day <- factor(size_suball3$day, levels = c('4', '5', '5bis', '6', '7', '11'), ordered = TRUE)
size_suball_track <- rbind(size_suball_track, size_suball3)
knitr::kable(size_suball_track)
rm(size_suball3)

ggplot(size_suball_track, aes(x = day, y = Nbr_of_cells)) +
    geom_col(aes(color = Analysis_step_name, fill = Analysis_step_name), position = position_dodge(), width = 0.6) +
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5)) +
    ggtitle("Number of cells in each sub-dataset\nafter doublets removal") +
    facet_grid(.~ origin)


## back to preprocessing analysis recluster cells after removing doublets ----------------------------------------------------
res <- seq(0.8, 1.6, 0.2)
allSO.list <- lapply(allSO.list,
                     function(SO){
                       SO<- NormalizeData(SO, verbose=FALSE)
                       SO<- FindVariableFeatures(SO, nfeatures=2000, verbose=FALSE)
                       SO<- ScaleData(SO, features=rownames(SO), verbose=FALSE)
                       SO<- RunPCA(SO, verbose=FALSE)
                       ElbowPlot(SO, ndims = 50) +
                         theme_minimal() +
                         theme(plot.title = element_text(hjust = 0.5)) +
                         geom_vline(aes(xintercept = 30), color="purple", size=1) +
                         ggtitle(paste0("ElbowPlot\n", SO@project.name))
                       SO<- FindNeighbors(SO, dims = 1:30, verbose=FALSE)                       
                       SO<- FindClusters(SO, resolution = res, random.seed = 11, verbose=FALSE)                       
                       SO <- RunUMAP(SO, dims = 1:30)
                       for (ares in res){
                         print(DimPlot(SO, group.by = paste0("RNA_snn_res.", ares), label = TRUE, reduction = "umap") +
                                 ggtitle(paste0("Resolution level: ", ares, "\n", SO@project.name)) +
                                 theme(plot.title = element_text(hjust=0.5)))
                       }
                       return(SO)
                     })


```


\clearpage

# Sources of unwanted variation

There are several sources of unwanted variation while doing a single cell RNA sequencing analysis.
Unwanted variation might come from a batch effect, but also from the cell-cycle state the cells are, or even the mitochondrial and ribosomal expression level.
I will check the batch effect on reference data only, given that I have only one batch for each sub-dataset of the lab.
Then, I will also check the cell-cycle effect on all the sub-datasets.\
Regarding the mitochondrial expression level, I will not study it given that I already applied thresholds on this variable.

## Batch effect

As already mentioned, from the lab, we have only one replicate for the sequencing data of day 04, day 06 and day 11.
The day 05 has been resequenced.
Looking at the two datasets (`lab_day_05` and `lab_day_05bis`), they are enough similar to be considered as batches of a technical replicate.

### Lab data on day 5

```{r plot_day5bef, , out.width="50%", fig.show='hold', warning=FALSE, message=FALSE, eval=TRUE}
# Get only reference sub-datasets
lab_datasets <- c("lab_day_05", "lab_day_05bis")
mergeDay5 <- merge(x = allSO.list$lab_day_05, y = allSO.list$lab_day_05bis, project = "lab_day_05", merge.data = T)
mergeDay5@meta.data[["dataset"]] <- "lab_day_05"
Idents(mergeDay5) <- mergeDay5$dataset
mergeDay5 <- NormalizeData(object = mergeDay5, normalization.method="LogNormalize", scale.factor=10000, verbose=FALSE)
mergeDay5 <- FindVariableFeatures(object = mergeDay5, nfeatures = 1000, verbose = FALSE)
mergeDay5 <- ScaleData(object = mergeDay5, verbose = FALSE)
mergeDay5 <- RunPCA(mergeDay5, npcs = 30, verbose = FALSE)

# function to do DimPlot entitled with sub-dataset name and if PCA is done before or after the regression
batch_DimPlot <- function(SO, when=NA){
  
  Idents(SO) <- SO$replicate
  DimPlot(SO) + ggtitle(paste0("Replicates of ", SO@project.name, "\nin the 2 first PCs ", when, " regression"))
}

batch_DimPlot(mergeDay5, when="before")
```

The two batches share a similar expression in the complete space of the two firsts PC.

Below, the batch correction is realized.
This step allows us to determine if whether the batch correction is required or not.

```{r plot_day5aft, out.height="20%", fig.show='hold', warning=FALSE, message=FALSE, eval=TRUE}
correctedDay5 <- ScaleData(mergeDay5, vars.to.regress="replicate", verbose=FALSE)
correctedDay5 <- RunPCA(correctedDay5)
batch_DimPlot(correctedDay5,  when="after")
```

The data after batch correction doesn't provide more information regarding the two firsts PC.
The batch correction will not be kept for the rest of the analysis.
The replicates of the day 05 will simply merged.

```{r change_day05, eval=TRUE}
allSO.list$lab_day_05 <- mergeDay5
allSO.list$lab_day_05bis <- NULL
SO.names <- names(allSO.list)
rm(mergeDay5, correctedDay5)
gc()
```

### Reference data

The reference data only will be tested on a batch effect regression.

```{r plot_anteRegress, out.width="50%", fig.show='hold', warning=FALSE, message=FALSE, eval=TRUE}
# Get only reference sub-datasets
ref_datasets <- c("ref_day_04", "ref_day_05", "ref_day_06", "ref_day_07")
ref_SO.list <- allSO.list[ref_datasets]

# function to do DimPlot entitled with sub-dataset name and if PCA is done before or after the regression
batch_DimPlot <- function(SO, when=NA){
  
  Idents(SO) <- SO$replicate
  DimPlot(SO) + ggtitle(paste0("Replicates of ", SO@project.name, "\nin the 2 first PCs ", when, " regression"))
}

future_lapply(ref_SO.list, batch_DimPlot, when="before", future.seed = 26)
```

The variation throughout data does not appear to be batch based.
Let see what occurs after regressing out on the batch value.

```{r batch_regression, out.height="20%", fig.show='hold', warning=FALSE, message=FALSE, eval=TRUE}
ref_SO.list <- future_lapply(ref_SO.list, ScaleData, vars.to.regress="replicate", verbose=FALSE, future.seed = 26)
ref_SO.list <- future_lapply(ref_SO.list, function(x) RunPCA(x, features=VariableFeatures(x)), future.seed = 26)
future_lapply(ref_SO.list, batch_DimPlot, when="after", future.seed = 26)
```

As observed previously, the data variation in the reference data is not lead by the replicates.

In conclusion, there is no need to apply the regression on the sub-datasets.

## cell-cycle effect

It is known that the cell-cycle state of the cells influence the pattern of gene expression.
This part is largely inspired by the [cell_cycle_vignette](https://satijalab.org/seurat/archive/v3.1/cell_cycle_vignette.html) provided by Seurat.

### Retrieve cell-cycle markers

After getting the list of the cell-cycle markers, I want to know if they have been detected in the sequencing.

```{r cc_markers, results='hold', eval=TRUE}
# A list of cell-cycle markers, from Tirosh et al, 2015, is loaded with Seurat
s.genes <- str_to_title(cc.genes$s.genes)
g2m.genes <- str_to_title(cc.genes$g2m.genes)

# Number of markers for the S and G2M phases
sapply(list("S_phase"=s.genes, "G2M_phase"=g2m.genes), length)

# Are cell-cycle markers in my data ?  
# check for the length of the intersection
sapply(allSO.list, function(x) length(intersect(rownames(x@assays$RNA), s.genes)))
sapply(allSO.list, function(x) length(intersect(rownames(x@assays$RNA), g2m.genes)))
```

Most of the markers have been sequenced in each of the sub-datasets.

### Determine the cell-cycle phase of the cells

Now, I can assign cell-cycle scores.
It is a score based on the expression of G2/M and S phase markers of each of the cells.

```{r cc_score, warning=FALSE, message=FALSE, eval=TRUE}
# Apply a cell cycle score
cc_SO.list <- future_lapply(allSO.list, CellCycleScoring, s.features = s.genes, g2m.features = g2m.genes, verbose=FALSE, future.seed = 26)
```

### Plots generation before cell-cycle regression

I create a generic function to plot the data in the first 2 PCs.
Each plot is titled depending whether the PC analysis rely on variables features or the cell-cycle markers, and giving the information the PCA has been done before or after the cell-cycle regression.

```{r funcPlot, eval=TRUE}
run_DimPlot <- function(SO, pca=NA, when=NA){
  Idents(SO) <- SO$Phase
  p <- DimPlot(SO) + ggtitle(paste0("PCA: ", pca, "\nRegression: ", when)) + theme(text = element_text(size = 10), plot.title = element_text(face = "plain"))
  return(p)
}
```

First call to that function.
The PCA on variable features has already be done during the `Pre-process Seurat objects` part.
Then, a new PCA is done on the cell-cycle markers, followed by a new call to the previously described function.

```{r plot_before, warning=FALSE, message=FALSE, eval=TRUE}
beforeAll <- future_lapply(cc_SO.list, run_DimPlot, pca="variable features", when="before", future.seed = 26)
names(beforeAll) <- SO.names

cc_SO.list <- future_lapply(cc_SO.list, RunPCA, features = c(s.genes, g2m.genes), verbose=FALSE, future.seed = 26)

beforeCCgenes <- future_lapply(cc_SO.list, run_DimPlot, pca="cell-cycle markers", when="before", future.seed = 26)
names(beforeCCgenes) <- SO.names
```

The plots are stored in lists, under the names of the sub-datasets.

### Run the cell-cycle regression

```{r cc_regression, warning=FALSE, message=FALSE, eval=TRUE}
# Regression
cc_SO.list <- future_lapply(cc_SO.list, function(x) ScaleData(x, vars.to.regress = c("S.Score", "G2M.Score"), verbose=FALSE), future.seed = 26)
```

### Plots generation after cell-cycle regression

```{r plot_after, warning=FALSE, message=FALSE, eval=TRUE}
# Plots generation
cc_SO.list <- future_lapply(cc_SO.list, RunPCA, verbose=FALSE, future.seed = 26)
afterAll <- future_lapply(cc_SO.list, run_DimPlot, pca="variable features", when="after", future.seed = 26)
names(afterAll) <- SO.names

cc_SO.list <- future_lapply(cc_SO.list, RunPCA, features = c(s.genes, g2m.genes), verbose=FALSE, future.seed = 26)
afterCCgenes <- future_lapply(cc_SO.list, run_DimPlot, pca="cell-cycle markers", when="after", future.seed = 26)
names(afterCCgenes) <- SO.names
```

### Plots display

```{r gc_13, include=FALSE, eval=TRUE}
gc()
```

```{r displayPlots, fig.show='hold', eval=TRUE}
displayPlot <- function(i){
  p1 <- beforeCCgenes[[i]]
  p2 <- beforeAll[[i]]
  p3 <- afterCCgenes[[i]]
  p4 <- afterAll[[i]]
  
  grid.arrange(p1,p2,p3,p4, nrow=2, top=textGrob(SO.names[i], hjust = 0.5, gp=gpar(fontsize = 18, fontface="bold")))
  
}

lapply(1:length(cc_SO.list), displayPlot)
```

It is expected to see a segregation between the cells according to their cell-cycle phase, as the PCA is done using the `s.genes` and `g2m.genes` lists.
Running a PCA on cell cycle markers reveals, unsurprisingly, that cells separate entirely by phase

...

\clearpage

# Conclusion

In this part, I orgnaized the datasets into sub-datasets relating to their origin, reference or lab, and to the day of the experiments they have been obtained.
Then, I cleaned the sub-datasets by studying the mitochondrial expression in the cells, and removing the outliers.
I also removed the doublets.
And, at the end I tested whether or not it is necessary to regress the sub-datasets over the batch effect or the cell-cycle phase.

All these sub-datasets have to be saved, to be used in the second part of the study, doing it by analysis.

```{r save01, eval=FALSE}
future_lapply(allSO.list, function(x) saveRDS(x, file=paste0(rdsObjects, "01_dailySO_", x@project.name, ".rds")), future.seed = 26)
```
